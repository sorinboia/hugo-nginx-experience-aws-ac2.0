[
{
	"uri": "/010_intro/005_workshop_path/",
	"title": "AWS Workshop environment",
	"tags": [],
	"description": "",
	"content": "There are two ways of doing the workshop, as part of an AWS event or on your own. Please click the relevant path.\n AWS Workshop On your own  "
},
{
	"uri": "/",
	"title": "Nginx Experience",
	"tags": [],
	"description": "",
	"content": "Welcome to AWS NGINX Meetup – From Application code to Customer Delivering modern applications has upped the needs to provide agile application services free from infrastructure constrains, while ensuring applications run faster, secured and at scale. Choosing the right architecture for application services allows organizations to innovate faster and role new services on any infrastructure or cloud architecture. From code development to customer consumption, NGINX deployed on AWS is helping millions of organizations deliver application infrastructure and services ensuring their application are fast, secure and run at scale.\nIntro to the workshop In this workshop you will experience how to deploy your application in an agile way, using NGINX Controller utilizing AWS compute, applying all the application services needed, such as load balancing, API management and application firewall in a fully automated way.\nDeploy NGINX infrastructure using Terraform Start by using automation and use Infrastructure as Code concepts to deploy the environment.\nDeploy your application with NGINX Unit Application Server Easley manage your application deployment lifecycle with the Nginx Unit Application Server\nIncrease availability, security and application performance with Kubernetes Nginx Ingress  Onboard with Caching and Load Balancing Deliver your application faster, using caching and compression to free application resources and improve user’s experience Intro to OpenTracing benefits Open Tracing provides the ability to monitor and resolve issues much faster when your application architecture becomes more complex Zero Trust - Applying Mutual TLS authentication with NGINX  The Nginx Controller  Publish the application APIs with NGINX Micro Gateway Deploy and publish your API endpoints with NGINX Micro Gateway within your Kubernetes environment Enhance APIs with JWT or Access Key token authentication Offload API authentication processes to your API gateway and concentrate Apply API rate limit and spike arrest Protect your APIs from abuse and enforce monetization controls Monitor your application performance Discover your application performance, understand and react when application SLAs are impacted  Security Secure you application with Nginx App Protect Web Application Firewall. Protect you application from fraudulent interactions and separate legitimate traffic from unwanted one with Nginx App Protect Web Application Firewall.\n"
},
{
	"uri": "/010_intro/aws_event/",
	"title": "...at an AWS event",
	"tags": [],
	"description": "",
	"content": "Running the workshop at an AWS Event Only complete this section if you are at an AWS hosted event (such as re:Invent, Kubecon, Immersion Day, or any other event hosted by an AWS employee). If you are running the workshop on your own, go to:\nStart the workshop on your own.\n\r"
},
{
	"uri": "/010_intro/selfpaced/",
	"title": "...on your own",
	"tags": [],
	"description": "",
	"content": "Running the workshop on your own Only complete this section if you are running the workshop on your own. If you are at an AWS hosted event (such as re:Invent, Kubecon, Immersion Day, etc), go to Start the workshop at an AWS event.\n\r"
},
{
	"uri": "/050_controller/010_accessing_controller/",
	"title": "Accessing the Nginx Controller",
	"tags": [],
	"description": "",
	"content": " The Nginx Controller has already been deployed with the terraform declaration, we need to find the public IP address.  cd terraform\rexport controller_ip=$(terraform state show \u0026quot;aws_instance.controller\u0026quot; | grep \u0026quot;public_ip\u0026quot; | grep -v \u0026quot;associate_public_ip_address\u0026quot; | cut -d'\u0026quot;' -f2)\rcurl -k -c cookie.txt -X POST --url \u0026quot;https://$controller_ip/api/v1/platform/login\u0026quot; --header 'Content-Type: application/json' --data '{\u0026quot;credentials\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;BASIC\u0026quot;,\u0026quot;username\u0026quot;: \u0026quot;nginx@f5.com\u0026quot;,\u0026quot;password\u0026quot;: \u0026quot;Admin2020\u0026quot;}}'\rexport controller_apikey=$(curl -k -sb cookie.txt -c cookie.txt https://$controller_ip/api/v1/platform/global | jq .currentStatus.agentSettings.apiKey | tr -d '\u0026quot;')\rterraform state show \u0026quot;aws_instance.controller\u0026quot; | grep \u0026quot;public_ip\u0026quot; | grep -v \u0026quot;associate_public_ip_address\u0026quot; | cut -d'\u0026quot;' -f2\rcd ..\rBrowse (using HTTPS) to the IP address of the Controller and verify you have access:  Username (email): nginx@f5.com\nPassword: Admin2020\n\r"
},
{
	"uri": "/030_unit/010_app_deployment_1/",
	"title": "App deployment",
	"tags": [],
	"description": "",
	"content": " Deploy the app  kubectl apply -f files/5ingress/1arcadia.yaml\rOutput\ndeployment.apps/arcadia-main created deployment.apps/arcadia-backend created deployment.apps/arcadia-app2 created deployment.apps/arcadia-app3 created service/arcadia-main created service/arcadia-backend created service/arcadia-app2 created service/arcadia-app3 created  Check that all is deployed and working as expected:  kubectl get pods\rOutput\nNAME READY STATUS RESTARTS AGE arcadia-app2-64ccdcdc97-2vn6w 1/1 Running 0 38s arcadia-app3-5d76bf776b-sj446 1/1 Running 0 38s arcadia-backend-bc96d5754-grwfn 1/1 Running 0 38s arcadia-main-5d9bc94d55-cc597 1/1 Running 0 39s  kubectl get svc -owide\rOutput\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR arcadia-app2 ClusterIP 172.20.215.142 none 80/TCP 23m app=arcadia-app2 arcadia-app3 ClusterIP 172.20.97.115 none 80/TCP 23m app=arcadia-app3 arcadia-main ClusterIP 172.20.102.115 none 80:32065/TCP 23m app=arcadia-main backend ClusterIP 172.20.84.9 none 80/TCP 5s app=arcadia-backend kubernetes ClusterIP 172.20.0.1 none 443/TCP 108m none  The application is not accessible yet. We will deploy the NGINX Ingress in the following sections.\n\r"
},
{
	"uri": "/010_intro/aws_event/010_aws_wokshop_portal_1/",
	"title": "AWS Workshop Portal",
	"tags": [],
	"description": "",
	"content": "This workshop creates an AWS account and a Cloud9 environment. You will need the Participant Hash provided upon entry, and your email address to track your unique session.\n Connect to the portal by clicking the button or browsing to https://dashboard.eventengine.run/. The following screen shows up.  Enter the provided hash in the text box. The button on the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.  Click on AWS Console on dashboard.  Accept the defaults and make sure the region is eu-central-1. Click on Open AWS Console. This will open AWS Console in a new browser tab.  "
},
{
	"uri": "/040_ingress/010_ingress_install_1/",
	"title": "Nginx Kubernetes Ingress Installation",
	"tags": [],
	"description": "",
	"content": "We are going to use the Nginx installation manifests based on the Nginx Ingress Controller installation guide. For simplicity - we have already prepared the installation in a single yaml file.\n Run the command bellow:  kubectl apply -f files/5ingress/nginx-ingress-install.yaml\rOutput\nnamespace/nginx-ingress created\rserviceaccount/nginx-ingress created\rclusterrole.rbac.authorization.k8s.io/nginx-ingress created\rclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress created\rsecret/default-server-secret created\rconfigmap/nginx-config created\rdeployment.apps/nginx-ingress created\rservice/nginx-ingress created\r Expose the Nginx Ingress Dashboard.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: v1\rkind: Service\rmetadata:\rname: dashboard-nginx-ingress\rnamespace: nginx-ingress\rannotations:\rservice.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026quot;tcp\u0026quot; spec:\rtype: LoadBalancer\rports:\r- port: 80\rtargetPort: 8080\rprotocol: TCP\rname: http\rselector:\rapp: nginx-ingress\rEOF\rCheck what we did so far is actually working:  kubectl get svc --namespace=nginx-ingress\rOutput\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rdashboard-nginx-ingress LoadBalancer 172.20.36.60 aeb592ad4011544219c0bc49581baa13-421891138.eu-central-1.elb.amazonaws.com 80:32044/TCP 11m\rnginx-ingress LoadBalancer 172.20.14.206 ab21b88fec1f445d98c79398abc2cd5d-961716132.eu-central-1.elb.amazonaws.com 80:30284/TCP,443:31110/TCP 5h35m\r Note the EXTERNAL-IP of the \u0026ldquo;dashboard-nginx-ingress\u0026rdquo;. This is the hostname that we are going to use in order to view the Nginx Dashboard.\nBrowse to the following location and verify you can see the dashboard: http://\u0026lt;DASHBOARD-EXTERNAL-IP\u0026gt;/dashboard.html\nNote the EXTERNAL-IP of the \u0026ldquo;nginx-ingress\u0026rdquo;. This is the hostname that we are going to use in order to publish the Arcadia web application.\nBrowse to the following location and verify that you receive a 404 status code: http://\u0026lt;INGRESS-EXTERNAL-IP\u0026gt;/\nPlease note that it might take some time for the DNS names to become available.\n\rSave the EXTERNAL-IPs as env variables for later use  export dashboard_nginx_ingress=$(kubectl get svc dashboard-nginx-ingress --namespace=nginx-ingress | tr -s \u0026quot; \u0026quot; | cut -d' ' -f4 | grep -v \u0026quot;EXTERNAL-IP\u0026quot;)\rexport nginx_ingress=$(kubectl get svc nginx-ingress --namespace=nginx-ingress | tr -s \u0026quot; \u0026quot; | cut -d' ' -f4 | grep -v \u0026quot;EXTERNAL-IP\u0026quot;)\r"
},
{
	"uri": "/070_cleanup/010_cleanup/",
	"title": "Remove configuration",
	"tags": [],
	"description": "",
	"content": " In order to delete the resources created during this workshop, run the commands below:  kubectl delete --all svc --namespace=nginx-ingress\rkubectl delete --all svc --namespace=default\rcd terraform\rterraform destroy --auto-approve\r Finally, delete the previously created Cloud9 stack in the CloudFormation console.  \rPlease note: This will also delete the Cloud9 IDE instance.\n\r"
},
{
	"uri": "/020_terraform/010_terraform_apply_1/",
	"title": "Terraform Apply",
	"tags": [],
	"description": "",
	"content": " Go to the \u0026ldquo;terraform\u0026rdquo; directory where we can find the terraform plan.  cd terraform Run the following commands, terraform plan will show us what it is going to be deployed in AWS by Terraform:  terraform init terraform plan Now let\u0026rsquo;s deploy the environment  terraform apply --auto-approve \rIt will take around 10 minutes for Terraform and AWS to finish the initial deployment.\nPlease continue to the next page to learn about Kubernetes and Amazon EKS basics.\n\r"
},
{
	"uri": "/060_security/010_waf_deployment/",
	"title": "Waf deployment",
	"tags": [],
	"description": "",
	"content": " Create the Nginx WAF config, which can be found in the \u0026ldquo;files/7waf/waf-config.yaml\u0026rdquo; file.  kubectl apply -f files/7waf/waf-config.yaml\rThe WAF policy is json based and from the example bellow, you can observe how all the configuration can be changed based on the application needs:\n{\r\u0026quot;name\u0026quot;: \u0026quot;nginx-policy\u0026quot;,\r\u0026quot;template\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;POLICY_TEMPLATE_NGINX_BASE\u0026quot; },\r\u0026quot;applicationLanguage\u0026quot;: \u0026quot;utf-8\u0026quot;,\r\u0026quot;enforcementMode\u0026quot;: \u0026quot;blocking\u0026quot;,\r\u0026quot;signature-sets\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;All Signatures\u0026quot;,\r\u0026quot;block\u0026quot;: false,\r\u0026quot;alarm\u0026quot;: true\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;High Accuracy Signatures\u0026quot;,\r\u0026quot;block\u0026quot;: true,\r\u0026quot;alarm\u0026quot;: true\r}\r],\r\u0026quot;blocking-settings\u0026quot;: {\r\u0026quot;violations\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;VIOL_RATING_NEED_EXAMINATION\u0026quot;,\r\u0026quot;alarm\u0026quot;: true,\r\u0026quot;block\u0026quot;: true\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;VIOL_HTTP_PROTOCOL\u0026quot;,\r\u0026quot;alarm\u0026quot;: true,\r\u0026quot;block\u0026quot;: true,\r\u0026quot;learn\u0026quot;: true\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;VIOL_FILETYPE\u0026quot;,\r\u0026quot;alarm\u0026quot;: true,\r\u0026quot;block\u0026quot;: true,\r\u0026quot;learn\u0026quot;: true\r},\r{\r\u0026quot;name\u0026quot;: \u0026quot;VIOL_COOKIE_MALFORMED\u0026quot;,\r\u0026quot;alarm\u0026quot;: true,\r\u0026quot;block\u0026quot;: false,\r\u0026quot;learn\u0026quot;: false\r}\r],\r\u0026quot;http-protocols\u0026quot;: [{\r\u0026quot;description\u0026quot;: \u0026quot;Body in GET or HEAD requests\u0026quot;,\r\u0026quot;enabled\u0026quot;: true,\r\u0026quot;learn\u0026quot;: true,\r\u0026quot;maxHeaders\u0026quot;: 20,\r\u0026quot;maxParams\u0026quot;: 500\r}],\r\u0026quot;filetypes\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;*\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;wildcard\u0026quot;,\r\u0026quot;allowed\u0026quot;: true,\r\u0026quot;responseCheck\u0026quot;: true\r}\r],\r\u0026quot;data-guard\u0026quot;: {\r\u0026quot;enabled\u0026quot;: true,\r\u0026quot;maskData\u0026quot;: true,\r\u0026quot;creditCardNumbers\u0026quot;: true,\r\u0026quot;usSocialSecurityNumbers\u0026quot;: true\r},\r\u0026quot;cookies\u0026quot;: [\r{\r\u0026quot;name\u0026quot;: \u0026quot;*\u0026quot;,\r\u0026quot;type\u0026quot;: \u0026quot;wildcard\u0026quot;,\r\u0026quot;accessibleOnlyThroughTheHttpProtocol\u0026quot;: true,\r\u0026quot;attackSignaturesCheck\u0026quot;: true,\r\u0026quot;insertSameSiteAttribute\u0026quot;: \u0026quot;strict\u0026quot;\r}\r],\r\u0026quot;evasions\u0026quot;: [{\r\u0026quot;description\u0026quot;: \u0026quot;%u decoding\u0026quot;,\r\u0026quot;enabled\u0026quot;: true,\r\u0026quot;learn\u0026quot;: false,\r\u0026quot;maxDecodingPasses\u0026quot;: 2\r}]}\r}\rDeploy ELK in order to be able to visualize and analyze the traffic going through the Nginx WAF  kubectl apply -f files/7waf/elk.yaml\rIn order to connect to our ELK pod, we will need to find the public address of this service:  kubectl get svc elk-web\rOutput\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\relk-web LoadBalancer 172.20.179.34 a28bd2d8c94214ae0b512274daa06211-2103709514.eu-central-1.elb.amazonaws.com 5601:32471/TCP,9200:32589/TCP,5044:31876/TCP 16h\r 4. Verify that ELK is up and running by browsing to: http://[ELK-EXTERNAL-IP]:5601/.\nPlease note that it might take some time for the DNS name to become available.\n\rNext, we need to change our deployment configuration so it includes the Nginx WAF  kubectl apply -f files/7waf/arcadia-main.yaml\rkubectl apply -f files/7waf/arcadia-app2.yaml\rkubectl apply -f files/7waf/arcadia-app3.yaml\rkubectl apply -f files/7waf/arcadia-backend.yaml\rAll of our services are protected and monitored.\n"
},
{
	"uri": "/010_intro/",
	"title": "Intro to the workshop",
	"tags": [],
	"description": "",
	"content": "Intro to the workshop\nThis workshop will provide guidelines on how to deploy an application from scratch in Amazon Elastic Kubernetes Service environment while protecting and enhancing the application availability and usability with Nginx solutions.\nFor this workshop we are going to use the \u0026ldquo;Arcadia Financial\u0026rdquo; application. The application is built with 4 different microservices that are deployed in the Kubernetes environment.\n Main - provides access to the web GUI of the application for use by browsers Backend - is a supporting microservice and provides support for the customer facing services only App2 - provides money transfer API based functionalities for both the Web app and third party consumer applications App3 - provides referral API based functionalities for both the Web app and third party consumer applications  By the end of the workshop the \u0026ldquo;Arcadia Financial\u0026rdquo; will be fully deployed and protected as described in the bellow diagram.\n"
},
{
	"uri": "/020_terraform/020_eks_1/",
	"title": "AWS EKS",
	"tags": [],
	"description": "",
	"content": "While you wait, you can review the Introduction section of the AWS EKS Workshop to learn about Kubernetes and Amazon EKS basics.\nManaged control plane Amazon EKS provides a scalable and highly-available control plane that runs across multiple AWS availability zones. The Amazon EKS service automatically manages the availability and scalability of the Kubernetes API servers and the etcd persistence layer for each cluster. Amazon EKS runs the Kubernetes control plane across three Availability Zones in order to ensure high availability, and it automatically detects and replaces unhealthy masters.\nManaged worker nodes Amazon EKS lets you create, update, or terminate worker nodes for your cluster with a single command. Managed node groups run nodes using the latest EKS-optimized AMIs in your AWS account while updates and terminations gracefully drain nodes to ensure your applications stay available.\n"
},
{
	"uri": "/010_intro/selfpaced/020_cloudformation_1/",
	"title": "Cloud Formation",
	"tags": [],
	"description": "",
	"content": " Click the bellow button and deploy the template:  \n Click Next and enter a unique \u0026ldquo;Stack name\u0026rdquo;\n  Click Next 2 times accepting all the defaults, but make sure the following is selected on the last screen:\n  Click Create stack.  Wait until the stack Status is CREATE_COMPLETE.\n"
},
{
	"uri": "/020_terraform/",
	"title": "Deploy NGINX infrastructure using Terraform",
	"tags": [],
	"description": "",
	"content": "Deploy NGINX infrastructure using Terraform\nWe will start by using Terraform to deploy the initial infrastructure which includes the Amazon Elastic Kubernetes Service and the EC2 instance for the Nginx Controller.\n"
},
{
	"uri": "/050_controller/020_microgateway/",
	"title": "Microgateway deployment",
	"tags": [],
	"description": "",
	"content": " Deploy the microgateway with the following configuration.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: microgateway\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: microgateway\rtemplate:\rmetadata:\rlabels:\rapp: microgateway\rspec:\rcontainers:\r- name: microgateway1\rimage: sorinboia/ngtest:3.4\rimagePullPolicy: Always\renv:\r- name: API_KEY\rvalue: $controller_apikey\r- name: CTRL_HOST\rvalue: $controller_ip\r- name: HOSTNAME\rvalue: microgateway1 ports:\r- containerPort: 80\rreadinessProbe:\rexec:\rcommand:\r- curl\r- 127.0.0.1:49151/api\rinitialDelaySeconds: 5\rperiodSeconds: 5\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: microgateway\rannotations: service.beta.kubernetes.io/aws-load-balancer-type: \u0026quot;nlb\u0026quot;\rservice.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026quot;tcp\u0026quot;\rspec:\rselector:\rapp: microgateway\rports:\r- port: 80\rtargetPort: 80\rname: http\r- port: 443\rtargetPort: 443\rname: https type: LoadBalancer\rEOF\rFrom now on we will only use the Controller GUI do to all of our configuration.\nThe end goal will be to expose and protect our APIs both internally within the cluster and externally to other programmers.\nLogin to the Nginx Controller web UI, click the \u0026ldquo;N\u0026rdquo; button in the upper left corner and go to \u0026ldquo;Infrastructure\u0026rdquo; -\u0026gt; \u0026ldquo;Instances\u0026rdquo;.  You will see the microgateway we just deployed listed. If it is not there wait for about 2 minutes, it might take a little bit of time for the instance to register.\nGet the EXTERNAL-IP of the microgateway service we just published, we will use it later within our config.  export microhost=$(kubectl get svc microgateway | tr -s \u0026quot; \u0026quot; | cut -d' ' -f4 | grep -v \u0026quot;EXTERNAL-IP\u0026quot;) \u0026amp;\u0026amp; echo $microhost\rOutput\naa2ba08e2b4024a85ba93aa32d0bafac-603500592.eu-central-1.elb.amazonaws.com\r "
},
{
	"uri": "/030_unit/020_nginx_unit/",
	"title": "Nginx Unit",
	"tags": [],
	"description": "",
	"content": "All of our pods are created based on the Nginx Unit application server. NGINX Unit is a dynamic application server, capable of running beside NGINX Plus and NGINX Open Source or standalone. Unit supports a RESTful JSON API, deploys configuration changes without service disruptions, and runs apps built with multiple languages and frameworks. Designed from scratch around the needs of your distributed applications, it lays the foundation for your service mesh.\nThe main features are:\nPolyglotism Configure all your applications with one streamlined interface:\n Uniform support for Go, Node.js, Perl, PHP, Python, and Ruby Applications written in different languages run on the same server Different versions of a language run side by side (PHP 5 and PHP 7, Python 2.7 and Python 3)  Programmability Adjust to your applications’ needs on-the-fly:\n Comprehensive RESTful API simplifies configuration JSON syntax provides visibility and transparency In‑memory updates reduce service disruptions Zero-downtime deployments facilitate seamless updates  Service Mesh Use NGINX Unit as the foundation for your service mesh:\n Integrated network stack for fast service-to-service communication Offload network configuration from application code to NGINX Unit Built-in SSL/TLS support  More information can be found here.\n"
},
{
	"uri": "/040_ingress/020_ingress_01/",
	"title": "Publish the app",
	"tags": [],
	"description": "",
	"content": "Expose all the application services and route traffic based on the HTTP path. We will start with a basic configuration.\n Expose Arcadia to the world.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: arcadia spec:\rrules:\r- host: $nginx_ingress\rhttp:\rpaths:\r- path: /\rbackend:\rserviceName: arcadia-main\rservicePort: 80\r- path: /api/\rbackend:\rserviceName: arcadia-app2\rservicePort: 80\r- path: /app3/\rbackend:\rserviceName: arcadia-app3\rservicePort: 80\rEOF\rNote how the various HTTP paths (/, /api/, /app3/) are routed by Ingress to the relevant K8s services.\nAt this stage the basic install is finished and all that\u0026rsquo;s left is to check the connectivity to the Arcadia web application. Get the public hostname of the exposed nginx-ingress service.\n Browse to the following location and verify that you can access the site: http://\u0026lt;INGRESS-EXTERNAL-IP\u0026gt;/\n  Login to the application using the following credentials:\n  Username: admin\nPassword: iloveblue\n\rAt the moment we still have two key features missing:\n We are serving only http, not https. We want our site to be fully secured therefore all communications need to be encrypted We are not actively monitoring the health of the pods through the data path   Take a look at the files/5ingress/2arcadia.yaml file. It increases the number of pods for our services to two - and also defines how the http health checks will looks like.\n  Apply this new configuration.\n  kubectl apply -f files/5ingress/2arcadia.yaml\rLook at the Nginx dashboard and click on \u0026ldquo;HTTP Upstreams\u0026rdquo;, you can see that right now that two HTTP upstreams have 2 members but no health checks are being done.  "
},
{
	"uri": "/060_security/020_testing/",
	"title": "Testing the Waf",
	"tags": [],
	"description": "",
	"content": "  Browse again to the Arcadia web app and verify that it is still working.\n  Let\u0026rsquo;s simulate a Cross Site Scripting (XSS) attack, and make sure it\u0026rsquo;s blocked:\n  https://\u0026lt;INGRESS-EXTERNAL-IP\u0026gt;/trading/index.php?a=%3Cscript%3Ealert(%27xss%27)%3C/script%3E\nEach of the blocked requests will generate a support ID, save it for later.\nBrowse to the ELK as before and click the \u0026ldquo;Discover\u0026rdquo; button:  Here, you\u0026rsquo;ll see all the request logs, allowed and blocked, sent by the Nginx WAF to ELK.\nLet\u0026rsquo;s look for the reason why our attack requests were blocked.\nAdd a filter with the support ID you have received as seen bellow:  In the right side of the panel, you can see the full request log and the reason why it was blocked.\nContinue and explore the visualization capabilities of Kibana and log information from Nginx WAF by looking into the next two sections bellow the \u0026ldquo;Discover\u0026rdquo; button (Visualize and Dashboard -\u0026gt; Overview).  "
},
{
	"uri": "/050_controller/030_configuration/",
	"title": "Build the configuration",
	"tags": [],
	"description": "",
	"content": " Create an environment  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Environments\u0026rdquo; -\u0026gt; \u0026ldquo;Create Environment\u0026rdquo; In all the fields, enter the following value: prod.\nClick on \u0026ldquo;View API Request\u0026rdquo;.\nAll the configuration on the Nginx Controller can be easlly automated with external orchestration systems, this view can help you in understanding how to generate the configuration API calls.\nThe output will look like this: Output\n{\r\u0026#34;metadata\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;prod\u0026#34;,\r\u0026#34;displayName\u0026#34;: \u0026#34;prod\u0026#34;,\r\u0026#34;description\u0026#34;: \u0026#34;prod\u0026#34;,\r\u0026#34;tags\u0026#34;: [\r\u0026#34;prod\u0026#34;\r]\r},\r\u0026#34;desiredState\u0026#34;: {}\r}\r\nClick \u0026ldquo;Submit\u0026rdquo;.\nCreate the Certificate:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Certs\u0026rdquo; -\u0026gt; \u0026ldquo;Create Cert\u0026rdquo;  Name: server-cert\nEnvironment: prod \nChose \u0026ldquo;Copy and paste PEM text\u0026rdquo;\nPrivate Key: Browse to https://raw.githubusercontent.com/sorinboia/nginx-experience-aws/master/certs_for_mtls/ca.key copy and paste.\nPublic Cert: Browse to https://raw.githubusercontent.com/sorinboia/nginx-experience-aws/master/certs_for_mtls/ca.pem copy and paste.\nSubmit\n Create the Gateway:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Gateways\u0026rdquo; -\u0026gt; \u0026ldquo;Create Gateway\u0026rdquo;  Name: api.arcadia.aws.cloud\nEnvironment: prod\nInstance Refs: Select All\nHostname: https://\u0026lt;EXTERNAL-IP OF THE \u0026quot;microgateway\u0026quot; SERVICE\u0026gt;\nCert Reference: server-cert\nSubmit\n Create the App:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Apps\u0026rdquo; -\u0026gt; \u0026ldquo;Create App\u0026rdquo;  Name: arcadia-api\nEnvironment: prod\nSubmit\n So far we have created an environment, uploaded the certificate/key that we will use for our HTTPS connection, created a gateway which represents our entry point into the API gateway and last defined a new application object.\n"
},
{
	"uri": "/030_unit/",
	"title": "Deploy your application with NGINX Unit Application Server",
	"tags": [],
	"description": "",
	"content": "We will deploy our application in the Kubernetes environment.\nAs stated before these are the 4 microservices which we will deploy.\n Main - provides access to the web GUI of the application for use by browsers Backend - is a supporting microservice and provides support for the customer facing services only App2 - provides money transfer API based functionalities for both the Web app and third party consumer applications App3 - provides referral API based functionalities for both the Web app and third party consumer applications  "
},
{
	"uri": "/020_terraform/030_eks_verification_1/",
	"title": "EKS Verification",
	"tags": [],
	"description": "",
	"content": "Wait for Terraform to finish and verify the deployment is working as expected and we are able to control the Kubernetes environment.\n We need to save the remote access config for the Kubernetes cluster locally:  mkdir ~/.kube/ terraform output \u0026gt; ~/.kube/config\rCheck and see that our cluster is up an running.\nBelow we should see our two K8s worker nodes:  kubectl get nodes\rOutput\nNAME STATUS ROLES AGE VERSION ip-10-0-2-32.eu-central-1.compute.internal Ready none 84s v1.15.10-eks-bac369 ip-10-0-3-217.eu-central-1.compute.internal Ready none 88s v1.15.10-eks-bac369  And the kube-system pods (this is the namespace for objects created by the Kubernetes system):\nAt the moment we have our setup deployed as it can be seen in the bellow diagram.\nChange the directory back to the original repo folder:  cd ..\r"
},
{
	"uri": "/040_ingress/030_health_https_1/",
	"title": "Enable https and monitoring",
	"tags": [],
	"description": "",
	"content": "In our next step we will finish this part of the configuration, we will implement the following:\n Enable health checks Enable https for the application and redirect http requests to https   Apply the configuration.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: v1\rkind: Secret\rmetadata:\rname: arcadia-tls\rnamespace: default\rdata:\rtls.crt: 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\rtls.key: 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\rtype: kubernetes.io/tls\r---\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: arcadia\rannotations:\rnginx.com/health-checks: \u0026quot;true\u0026quot;\ringress.kubernetes.io/ssl-redirect: \u0026quot;true\u0026quot;\rspec:\rtls:\r- hosts:\r- $nginx_ingress\rsecretName: arcadia-tls\rrules:\r- host: $nginx_ingress\rhttp:\rpaths:\r- path: /\rbackend:\rserviceName: arcadia-main\rservicePort: 80\r- path: /api/\rbackend:\rserviceName: arcadia-app2\rservicePort: 80\r- path: /app3/\rbackend:\rserviceName: arcadia-app3\rservicePort: 80\rEOF\rBrowse to the Arcadia website with http and you will be automatically redirected to https.\nLook at the Nginx dashboard and observe that Nginx has started monitoring the pods.  "
},
{
	"uri": "/010_intro/selfpaced/030_iam_role_1/",
	"title": "IAM Role",
	"tags": [],
	"description": "",
	"content": "  Follow this deep link to find your Cloud9 EC2 instance\n  Select the instance, then choose Actions / Instance Settings / Attach/Replace IAM Role\n  Choose eksworkshop-admin from the IAM Role drop down, and select Apply  "
},
{
	"uri": "/040_ingress/040_application_performance_1/",
	"title": "Speed up application performance and enable caching",
	"tags": [],
	"description": "",
	"content": " Apply the bellow configuration. We are telling Nginx to create a caching entity that will be used by our Ingress.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rkind: ConfigMap\rapiVersion: v1\rmetadata:\rname: nginx-config\rnamespace: nginx-ingress\rdata:\rproxy-protocol: \u0026quot;True\u0026quot;\rreal-ip-header: \u0026quot;proxy_protocol\u0026quot;\rset-real-ip-from: \u0026quot;0.0.0.0/0\u0026quot;\rhttp-snippets : |\rproxy_cache_path /var/tmp/a levels=1:2 keys_zone=my_cache:10m max_size=100m inactive=60m use_temp_path=off;\rEOF\rConfigure the Nginx Ingress to start using it and start caching.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: arcadia\rannotations:\rnginx.com/health-checks: \u0026quot;true\u0026quot;\ringress.kubernetes.io/ssl-redirect: \u0026quot;true\u0026quot;\rnginx.org/server-snippets: |\rproxy_ignore_headers X-Accel-Expires Expires Cache-Control;\rproxy_cache_valid any 30s;\rnginx.org/location-snippets: | proxy_cache my_cache;\radd_header X-Cache-Status \\$upstream_cache_status;\rspec:\rtls:\r- hosts:\r- $nginx_ingress\rsecretName: arcadia-tls\rrules:\r- host: $nginx_ingress\rhttp:\rpaths:\r- path: /\rbackend:\rserviceName: arcadia-main\rservicePort: 80\r- path: /api/\rbackend:\rserviceName: arcadia-app2\rservicePort: 80\r- path: /app3/\rbackend:\rserviceName: arcadia-app3\rservicePort: 80\rEOF\rWe have two simple indicators to check that all is working:   First if we open the browser developer tools we can see a new http header in the response called \u0026ldquo;X-Cache-Status\u0026rdquo;.\nIf the response was taken from the cache it will have a value of \u0026ldquo;HIT\u0026rdquo; otherwise if it was server by the server the value will be \u0026ldquo;MISS\u0026rdquo; The second options is to look at the Nginx Dashboard -\u0026gt; Caches and observe the HIT ration and traffic served  "
},
{
	"uri": "/010_intro/040_cloud9_1/",
	"title": "Cloud 9",
	"tags": [],
	"description": "",
	"content": " Open the Cloud9 console, and click on Open IDE. This will open a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and a terminal. We will be using it for the rest of the workshop.  \rAd blockers, javascript disablers, and tracking blockers should be disabled for the cloud9 domain, or connecting to the workspace might be impacted. Cloud9 requires third-party-cookies. You can whitelist the specific domains.\n\rRun the following command to install all the software tools required to run the workshop:  labs/eks/install.sh\rThe output of the script should show: Output\nkubectl in path\rjq in path\renvsubst in path\raws in path\raws-iam-authenticator in path\rterraform in path\r\n Disable temporary credentials:   In the Cloud9 IDE click the gear icon (in top right corner), or click to open a new tab and choose \u0026ldquo;Open Preferences\u0026rdquo; Select AWS SETTINGS Turn off AWS managed temporary credentials Close the Preferences tab    Validate the IAM role:  Use the GetCallerIdentity CLI command to validate that the Cloud9 IDE is using the correct IAM role.\naws sts get-caller-identity --query Arn | grep eksworkshop-admin -q \u0026amp;\u0026amp; echo \u0026quot;IAM role valid\u0026quot; || echo \u0026quot;IAM role NOT valid\u0026quot;\rIf the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\n Clone the Workshop Repo:  git clone https://github.com/sorinboia/nginx-experience-aws\rcd nginx-experience-aws/\r "
},
{
	"uri": "/050_controller/040_apis/",
	"title": "Import the OpenApi definition",
	"tags": [],
	"description": "",
	"content": "Next we are going to publish the application APIs to the world.\nThere are two ways of creating this configuration, the first one is manual similar to the way we performed the configuration until this point and the second one is described bellow.\nAs part of their development cycle, the developers of the Arcadia application are generating an OpenApi specification to describe their APIs.\nWe are going to use this API specification in order to publish the services to the world.\n Run the following curl commands.  curl -k -sc cookie.txt -X POST --url \u0026quot;https://$controller_ip/api/v1/platform/login\u0026quot; --header 'Content-Type: application/json' --data '{\u0026quot;credentials\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;BASIC\u0026quot;,\u0026quot;username\u0026quot;: \u0026quot;nginx@f5.com\u0026quot;,\u0026quot;password\u0026quot;: \u0026quot;Admin2020\u0026quot;}}' curl -k -sb cookie.txt -c cookie.txt --location --request PUT \u0026quot;https://$controller_ip/api/v1/services/api-definitions/arcadia-api/versions/v1\u0026quot; --header 'Content-Type: application/json' --header 'Content-Type: text/plain' --data \u0026quot;@files/6controller/arcadia_api_spec.json\u0026quot;\rWe have just uploaded the OpenApi spec to the Nginx Controller.\n Go to \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;APIs\u0026rdquo;.\nYou can see the \u0026ldquo;Arcadia API\u0026rdquo; definition listed.\n  Check the DNS name of the backend servers we need to point our APIs to:\n  kubectl get svc\rOutput\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rarcadia-app2 ClusterIP 172.20.103.189 none 80/TCP 171m\rarcadia-app3 ClusterIP 172.20.238.13 none 80/TCP 171m\rarcadia-backend ClusterIP 172.20.228.83 none 80/TCP 109m\rarcadia-main ClusterIP 172.20.166.2 none 80/TCP 7s\rbackend ClusterIP 172.20.44.133 none 80/TCP 171m\rkubernetes ClusterIP 172.20.0.1 none 443/TCP 8h\rmicrogateway LoadBalancer 172.20.81.110 a2fa7314165114fb9b16ebd92a890078-367878391.eu-central-1.elb.amazonaws.com 80:32293/TCP,443:32428/TCP 12m\r We are interested in \u0026ldquo;main\u0026rdquo; and \u0026ldquo;app2\u0026rdquo; and their DNS names are arcadia-main and arcadia-app2.\n"
},
{
	"uri": "/040_ingress/",
	"title": "Increase availability, security and application performance with Kubernetes Nginx Ingress",
	"tags": [],
	"description": "",
	"content": "Previously we have deployed the application but did not expose the services.\nWe need to be able to route the requests to the relevant service.\nNginx Kubernetes Ingress to save the day! :) The NGINX Ingress Controller for Kubernetes provides enterprise‑grade delivery services for Kubernetes applications, with benefits for users of both NGINX Open Source and NGINX Plus. With the NGINX Ingress Controller for Kubernetes, you get basic load balancing, SSL/TLS termination, support for URI rewrites, and upstream SSL/TLS encryption. NGINX Plus users additionally get session persistence for stateful applications and JSON Web Token (JWT) authentication for APIs.\n"
},
{
	"uri": "/040_ingress/050_open_tracing_1/",
	"title": "What is open tracing",
	"tags": [],
	"description": "",
	"content": "So, what is OpenTracing? It’s a vendor-agnostic API to help developers easily instrument tracing into their code base. It’s open because no one company owns it. In fact, many tracing tooling companies are getting behind OpenTracing as a standardized way to instrument distributed tracing.\nOpenTracing wants to form a common language around what a trace is and how to instrument them in our applications. In OpenTracing, a trace is a directed acyclic graph of Spans with References that may look like this :\n[Span A] ←←←(the root span)\r|\r+------+------+\r| |\r[Span B] [Span C] ←←←(Span C is a `ChildOf` Span A)\r| |\r[Span D] +---+-------+\r| |\r[Span E] [Span F] \u0026gt;\u0026gt;\u0026gt; [Span G] \u0026gt;\u0026gt;\u0026gt; [Span H]\r↑\r↑\r↑\r(Span G `FollowsFrom` Span F)\rThis allows us to model how our application calls out to other applications, internal functions, asynchronous jobs, etc. All of these can be modeled as Spans, as we’ll see below.\nFor example, if I have a consumer website where a customer places orders, I make a call to my payment system and my inventory system before asynchronously acknowledging the order. I can trace the entire order process through every system with an OpenTracing library and can render it like this:\nOpen Tracing tracing is becoming more and more important because software systems are becoming more and more distributed and complex. We need ways to correlate them so that we can understand what is happening inside them. When we know what’s happening inside them, we can quickly hunt down defects and other incidents. Good distributed tracing tooling can save you hours or days of frustration.\n"
},
{
	"uri": "/040_ingress/060_mtls_1/",
	"title": "Mutual TLS",
	"tags": [],
	"description": "",
	"content": "Enabling MTLS on our Nginx Ingress Controller is quite simple, we are going to add two lines to the existing config.\n We are going to add the bellow directives to our Ingress configuration, This will enable MTLS while using the pre uploaded ca.pem certificate.  ssl_client_certificate /etc/ssl/mycerts/ca.pem;\nssl_verify_client on;\n\rApply the new configuration:\ncat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: arcadia\rannotations:\rnginx.com/health-checks: \u0026quot;true\u0026quot; ingress.kubernetes.io/ssl-redirect: \u0026quot;true\u0026quot;\rnginx.com/sticky-cookie-services: \u0026quot;serviceName=arcadia-main srv_id expires=1h path=/\u0026quot;\rnginx.org/server-snippets: |\rproxy_ignore_headers X-Accel-Expires Expires Cache-Control;\rproxy_cache_valid any 30s;\rssl_client_certificate /etc/ssl/mycerts/ca.pem;\rssl_verify_client on;\rnginx.org/location-snippets: | proxy_cache my_cache;\radd_header X-Cache-Status \\$upstream_cache_status;\rspec:\rtls:\r- hosts:\r- $nginx_ingress\rsecretName: arcadia-tls\rrules:\r- host: $nginx_ingress\rhttp:\rpaths:\r- path: /\rbackend:\rserviceName: arcadia-main\rservicePort: 80\r- path: /api/\rbackend:\rserviceName: arcadia-app2\rservicePort: 80\r- path: /app3/\rbackend:\rserviceName: arcadia-app3\rservicePort: 80\rEOF\r Browse to the Arcadia site again, and you\u0026rsquo;ll see that you can\u0026rsquo;t access it since you don\u0026rsquo;t have the client certificate.\n  Verify this is actually working by running the bellow command, it will use the client cert/key pair on the Cloud9 instance to authenticate:\n  curl -v -k \\\r--key certs_for_mtls/01-alice.key \\\r--cert certs_for_mtls/01-alice.pem \\\rhttps://$nginx_ingress/ \\\r| grep 'Welcome'\rWe are finished with this part of our experience and achieved the bellow environment.\nBefore moving forward reapply the ingress configuration without the two lines we just added.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: extensions/v1beta1\rkind: Ingress\rmetadata:\rname: arcadia\rannotations:\rnginx.com/health-checks: \u0026quot;true\u0026quot; ingress.kubernetes.io/ssl-redirect: \u0026quot;true\u0026quot;\rnginx.com/sticky-cookie-services: \u0026quot;serviceName=arcadia-main srv_id expires=1h path=/\u0026quot;\rnginx.org/server-snippets: |\rproxy_ignore_headers X-Accel-Expires Expires Cache-Control;\rproxy_cache_valid any 30s; nginx.org/location-snippets: | proxy_cache my_cache;\radd_header X-Cache-Status \\$upstream_cache_status;\rspec:\rtls:\r- hosts:\r- $nginx_ingress\rsecretName: arcadia-tls\rrules:\r- host: $nginx_ingress\rhttp:\rpaths:\r- path: /\rbackend:\rserviceName: arcadia-main\rservicePort: 80\r- path: /api/\rbackend:\rserviceName: arcadia-app2\rservicePort: 80\r- path: /app3/\rbackend:\rserviceName: arcadia-app3\rservicePort: 80\rEOF\r"
},
{
	"uri": "/050_controller/",
	"title": "Nginx Controller",
	"tags": [],
	"description": "",
	"content": "We have finished the first part of the publishing our application, now we want to publish our APIs to be used by third party organizations. We will acomplish this using two components:\n Nginx Controller which will be used as an API Management Nginx Container will be the API Microgateway which will reside within the Kubernetes environment  "
},
{
	"uri": "/050_controller/050_publish_apis/",
	"title": "Publish the external APIs",
	"tags": [],
	"description": "",
	"content": "We are now going to publish the previously imported OpenApi definition.\n Publish the APIs:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;APIs\u0026rdquo; -\u0026gt; Click \u0026ldquo;arcadia-api\u0026rdquo; -\u0026gt; Click \u0026ldquo;Add Published API\u0026rdquo; Fill in the data as described bellow   Name: arcadia-published-api\nClick Next\n  Environment: prod\nApp: arcadia-api\nGateways: api.arcadia.aws.cloud\nClick Next\n We will now add the components that represent the workload for the main app that traffic will be sent to.\nClick \u0026ldquo;Add New\u0026rdquo; on the \u0026ldquo;Components\u0026rdquo; column and fill the data as described bellow   Name: arcadia-main-component\nClick Next\n  Workload Group Name: arcadia-main-wl\nURI: http://arcadia-main\nClick Submit\n Add the second component that represents the workload for app2\nClick \u0026ldquo;Add New\u0026rdquo; on the \u0026ldquo;Components\u0026rdquo; column and fill the data as described bellow   Name: arcadia-app2-component\nClick Next\n  Workload Group Name: arcadia-app2-wl\nURI: http://arcadia-app2 Click Submit\n Move the routes that start with \u0026ldquo;/api\u0026rdquo; in the \u0026ldquo;Unrouted\u0026rdquo; to the \u0026ldquo;Components\u0026rdquo; column under the \u0026ldquo;arcadia-app2-component\u0026rdquo;. Move the remaining routes under \u0026ldquo;arcadia-main-component\u0026rdquo; and click Submit  We have finished publishing the API all is left is to test it. Run the bellow curl command, you should receive a success message and if you go to the main Arcadia application and refresh the page you will be able to see the transaction we just did in the \u0026ldquo;Transfer History\u0026rdquo; section.  curl -k --location --request POST https://$microhost/api/rest/execute_money_transfer.php --header 'Content-Type: application/json' --data-raw '{\u0026quot;amount\u0026quot;:\u0026quot;77\u0026quot;,\u0026quot;account\u0026quot;:\u0026quot;2075894\u0026quot;,\u0026quot;currency\u0026quot;:\u0026quot;EUR\u0026quot;,\u0026quot;friend\u0026quot;:\u0026quot;Alfredo\u0026quot;}'\rOutput\n{\u0026#34;name\u0026#34;:\u0026#34;Alfredo\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;amount\u0026#34;:\u0026#34;77\u0026#34;, \u0026#34;currency\u0026#34;:\u0026#34;EUR\u0026#34;, \u0026#34;transid\u0026#34;:\u0026#34;944962065\u0026#34;, \u0026#34;msg\u0026#34;:\u0026#34;The money transfer has been successfully completed \u0026#34;}\r "
},
{
	"uri": "/050_controller/060_add_auth/",
	"title": "Add authentication to the API",
	"tags": [],
	"description": "",
	"content": "Our APIs are published and now we will want to add authentication based on API Keys\n Create an Identity Provider:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Identity Provider\u0026rdquo; -\u0026gt; \u0026ldquo;Create an Identity Provider\u0026rdquo;  Name: api-protect\nEnvironment: prod\nType: API Key\n Under \u0026ldquo;API Clients\u0026rdquo;\n Name: test-client\nKey: 1234567890\nClick Submit\n Go the the edit mode of APIs we have previously published  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;APIs\u0026rdquo; -\u0026gt; Click \u0026ldquo;arcadia-api\u0026rdquo; -\u0026gt; Click the edit icon Attach the new Identity Provider to the APIs   Click \u0026ldquo;Routing\u0026rdquo;\nClick the edit icon of \u0026ldquo;arcadia-app2-component\u0026rdquo; component security setting\n Click \u0026ldquo;Add Authentication\u0026rdquo; and fill in the fields   Identity Provider: api-protect\nCredential Location: Header\nCredentials Value: apikey\nClick Done\nClick Submit\nClick Submit again on the \u0026ldquo;Edit Published API\u0026rdquo; page\n Check that the authentication is failing when not providing an API key  curl -k --location --request POST https://$microhost/api/rest/execute_money_transfer.php --header 'Content-Type: application/json' --data-raw '{\u0026quot;amount\u0026quot;:\u0026quot;77\u0026quot;,\u0026quot;account\u0026quot;:\u0026quot;2075894\u0026quot;,\u0026quot;currency\u0026quot;:\u0026quot;EUR\u0026quot;,\u0026quot;friend\u0026quot;:\u0026quot;Alfredo\u0026quot;}'\rOutput\n\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;401 Authorization Required\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;401 Authorization Required\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt;\r\u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx\u0026lt;/center\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r Try again but this time including the API key and the test-client value  curl -k --location --request POST https://$microhost/api/rest/execute_money_transfer.php --header \u0026quot;apikey: 1234567890\u0026quot; --header 'Content-Type: application/json' --data-raw '{\u0026quot;amount\u0026quot;:\u0026quot;77\u0026quot;,\u0026quot;account\u0026quot;:\u0026quot;2075894\u0026quot;,\u0026quot;currency\u0026quot;:\u0026quot;EUR\u0026quot;,\u0026quot;friend\u0026quot;:\u0026quot;Alfredo\u0026quot;}'\rOutput\n{\u0026#34;name\u0026#34;:\u0026#34;Alfredo\u0026#34;, \u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;,\u0026#34;amount\u0026#34;:\u0026#34;77\u0026#34;, \u0026#34;currency\u0026#34;:\u0026#34;EUR\u0026#34;, \u0026#34;transid\u0026#34;:\u0026#34;772067369\u0026#34;, \u0026#34;msg\u0026#34;:\u0026#34;The money transfer has been successfully completed \u0026#34;}\r "
},
{
	"uri": "/060_security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "In our final part of the workshop, we will implement a per-pod Web Application Firewall.\nThe Nginx WAF will allow to improve the application security posture, especially against OWASP Top 10 attacks.\nIn our scenario, since we decided our Nginx WAF to be enabled on a per-pod basis, we will be able to protect all the traffic coming into the pod regardless of where it is originating from (external or internal to the Kubernetes cluster).\nWe\u0026rsquo;ll be able to bring security closer to the application and the development cycle and integrate it into CI/CD pipelines.\nThis will allow to minimize false positives, since the WAF policy becomes a part of the application and is always tested as such.\n"
},
{
	"uri": "/070_cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "If you have run this workshop on your own environment you might need to delete the created resources. Please follow the steps in the next part.\n"
},
{
	"uri": "/080_feedback/",
	"title": "Feedback",
	"tags": [],
	"description": "",
	"content": "The code in this repo is under constant development.\nFor any feedback or suggestions, either open an Issue on GitHub, or contact:\nSorin for any NGINX related topics\nArtiom for any AWS related topics\n"
},
{
	"uri": "/050_controller/070_summary/",
	"title": "Summary",
	"tags": [],
	"description": "",
	"content": "All of our microgateway API configuration is complete. We have published external APIs and are able to route, authenticate and view statistics for traffic coming from API clients. We have achieved the bellow architecture:\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]